{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HZ8ynzVpISZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab234cb8-ccf8-459c-d721-ac5d50fdf463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, Perceptron\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import SGDClassifier, Perceptron, LogisticRegression, RidgeClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Y A UNE CELLULE EN BAS QUI IMPORTE LES DONNÉES DES CSV C'EST PLUS SIMPLE DE LANCER À PARTIR DE ÇA (et la cellule des import évidemment)"
      ],
      "metadata": {
        "id": "rC6BlwxHIiWM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BGHZvHjqIqvR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWWQai3lISZV"
      },
      "source": [
        "# Extraction des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wqN-XKc4ISZX"
      },
      "outputs": [],
      "source": [
        "def get_xml_content(file):\n",
        "\n",
        "    with open(file, \"r\") as f:\n",
        "        soup = BeautifulSoup(f, \"xml\")\n",
        "\n",
        "    return soup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UX6gTVeKISZY"
      },
      "outputs": [],
      "source": [
        "def extract_train_info(doc):\n",
        "\n",
        "    doc_dico = {}\n",
        "\n",
        "    # Extraire l'id\n",
        "    doc_id = doc.get(\"id\")\n",
        "    doc_dico[\"id\"] = doc_id\n",
        "\n",
        "   # Extraire l'évaluation\n",
        "    evaluation = doc.find(\"EVALUATION\")\n",
        "    evaluation_parti = evaluation.find(\"EVAL_PARTI\")\n",
        "    doc_dico[\"nombre\"] = evaluation_parti.get(\"nombre\")\n",
        "\n",
        "    parti = evaluation_parti.find(\"PARTI\")\n",
        "    doc_dico[\"valeur\"] = parti.get(\"valeur\")\n",
        "    doc_dico[\"confiance\"] = parti.get(\"confiance\")\n",
        "\n",
        "    # Extraire le texte\n",
        "    texte = doc.find(\"p\")\n",
        "    doc_dico[\"texte\"] = texte.text\n",
        "\n",
        "    return doc_dico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "izZZOCQJISZa"
      },
      "outputs": [],
      "source": [
        "def get_test_labels(file):\n",
        "\n",
        "    with open(file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    lines = [line.split(\"\\t\") for line in lines]\n",
        "    labels = {line[0]: line[1].strip() for line in lines}\n",
        "\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-irf4n-HISZc"
      },
      "source": [
        "J'ai remarqué plus tard que deux textes de l'ensemble de test n'avaient pas de labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5b3l20UFISZc",
        "outputId": "0dbd5efd-e0a3-46d7-c138-dd876193b7f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1175\n",
            "4574\n"
          ]
        }
      ],
      "source": [
        "labels = get_test_labels(\"data/ref/deft09_parlement_ref_fr.txt\")\n",
        "for key, value in labels.items():\n",
        "    if value == \"\":\n",
        "        print(key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkcFC9xaISZd"
      },
      "source": [
        "Je fais donc en sorte lorsque j'obtiens la df que si la valeur est égale à \"\" alors je ne prends pas le texte en compte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W5SdjBilISZe"
      },
      "outputs": [],
      "source": [
        "def extract_test_info(doc):\n",
        "\n",
        "    doc_dico = {}\n",
        "\n",
        "    doc_id = doc.get(\"id\")\n",
        "    doc_dico[\"id\"] = doc_id\n",
        "\n",
        "    texte = doc.find(\"p\")\n",
        "    doc_dico[\"texte\"] = texte.text\n",
        "\n",
        "    return doc_dico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QqgtOvPzISZg"
      },
      "outputs": [],
      "source": [
        "def get_train_and_test_df(train_file, test_file, ref_file):\n",
        "\n",
        "    soup_train = get_xml_content(train_file)\n",
        "    soup_test = get_xml_content(test_file)\n",
        "\n",
        "    test_labels = get_test_labels(ref_file)\n",
        "\n",
        "    all_docs = []\n",
        "    for doc in soup_train.find_all(\"doc\"):\n",
        "        doc_dico = extract_train_info(doc)\n",
        "        all_docs.append(doc_dico)\n",
        "\n",
        "    df_train = pd.DataFrame(all_docs)\n",
        "\n",
        "    all_docs = []\n",
        "    for doc in soup_test.find_all(\"doc\"):\n",
        "        doc_dico = extract_test_info(doc)\n",
        "        if test_labels[doc_dico[\"id\"]] == \"\": # pour les deux textes sans label dans le fichier de référence\n",
        "            continue\n",
        "        doc_dico[\"valeur\"] = test_labels[doc_dico[\"id\"]]\n",
        "        all_docs.append(doc_dico)\n",
        "\n",
        "    df_test = pd.DataFrame(all_docs)\n",
        "\n",
        "    return df_train, df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mvLNSzL6ISZg"
      },
      "outputs": [],
      "source": [
        "df_train, df_test = get_train_and_test_df(\"data/train/deft09_parlement_appr_fr.xml\", \"data/test/deft09_parlement_test_fr.xml\", \"data/ref/deft09_parlement_ref_fr.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JFDKaVOISZh"
      },
      "outputs": [],
      "source": [
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djslUV0FISZh"
      },
      "outputs": [],
      "source": [
        "df_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv9vUdsnISZj"
      },
      "source": [
        "# Présentation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0N4l7snISZj"
      },
      "source": [
        "## - Doublons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InUywardISZj"
      },
      "source": [
        "### - Combien de doublons ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwuUFCC1ISZj"
      },
      "outputs": [],
      "source": [
        "train_texts = df_train[\"texte\"].to_list()\n",
        "test_texts = df_test[\"texte\"].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K46le1VsISZj"
      },
      "outputs": [],
      "source": [
        "def check_copy(train_texts, test_texts):\n",
        "\n",
        "    num_copy = 0\n",
        "\n",
        "    for text in train_texts:\n",
        "        if text in test_texts:\n",
        "            num_copy += 1\n",
        "\n",
        "    return num_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUCddTevISZj"
      },
      "outputs": [],
      "source": [
        "num_copy = check_copy(train_texts, test_texts)\n",
        "print(f\"Il y a {num_copy} textes en commun entre les données d'entraînement et de test.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqZh0raPISZk"
      },
      "source": [
        "### - On veut atteindre une distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA7NfYkNISZk"
      },
      "source": [
        "Répartition des sets = 60 et 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAMzfp0CISZk"
      },
      "outputs": [],
      "source": [
        "def get_distribution(df_train, df_test):\n",
        "\n",
        "    total_texts = len(df_train) + len(df_test)\n",
        "    percentage_train = len(df_train) / total_texts * 100\n",
        "    percentage_test = len(df_test) / total_texts * 100\n",
        "\n",
        "    return percentage_train, percentage_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SV2KxFzhISZl"
      },
      "outputs": [],
      "source": [
        "percentage_train, percentage_test = get_distribution(df_train, df_test)\n",
        "print(f\"Pourcentage de textes d'entraînement : {percentage_train:.2f}%\")\n",
        "print(f\"Pourcentage de textes de test : {percentage_test:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EWEM2x5ISZl"
      },
      "source": [
        "On veut enlever les doublons tout en essayant de conserver cette distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1EPNpJBISZl"
      },
      "outputs": [],
      "source": [
        "def balance_distribution(df_train, df_test):\n",
        "\n",
        "    train_texts = df_train[\"texte\"].to_list()\n",
        "    test_texts = df_test[\"texte\"].to_list()\n",
        "\n",
        "    for text in train_texts:\n",
        "        if text in test_texts:\n",
        "            percentage_train, percentage_test = get_distribution(df_train, df_test)\n",
        "            difference_train = abs(percentage_train - 60)\n",
        "            difference_test = abs(percentage_test - 40)\n",
        "\n",
        "            if difference_train > difference_test:\n",
        "                df_test = df_test[df_test[\"texte\"] != text]\n",
        "            else:\n",
        "                df_train = df_train[df_train[\"texte\"] != text]\n",
        "\n",
        "    return df_train, df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBrGREruISZm"
      },
      "outputs": [],
      "source": [
        "df_train, df_test = balance_distribution(df_train, df_test)\n",
        "percentage_train, percentage_test = get_distribution(df_train, df_test)\n",
        "print(f\"Pourcentage de textes d'entraînement : {percentage_train:.2f}%\")\n",
        "print(f\"Pourcentage de textes de test : {percentage_test:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT2NzuNrISZn"
      },
      "outputs": [],
      "source": [
        "train_texts = df_train[\"texte\"].to_list()\n",
        "test_texts = df_test[\"texte\"].to_list()\n",
        "num_copy = check_copy(train_texts, test_texts)\n",
        "print(f\"Il y a {num_copy} textes en commun entre les données d'entraînement et de test.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ9T12TfISZn"
      },
      "source": [
        "### - Retirer jusqu'à 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeAlocsdISZn"
      },
      "source": [
        "Nous n'avons plus de doublons mais nous restons quelque peu éloignés de la distribution 60/40. On va donc retirer des textes de l'ensemble de test pour arriver à la distribution souhaitée."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf9lVkLOISZn"
      },
      "outputs": [],
      "source": [
        "def reach_desired_distribution(df_train, df_test):\n",
        "\n",
        "    while True:\n",
        "        percentage_train, percentage_test = get_distribution(df_train, df_test)\n",
        "\n",
        "        if percentage_train >= 60:\n",
        "            break\n",
        "        else:\n",
        "            df_test = df_test.drop(df_test.index[0])\n",
        "\n",
        "\n",
        "    return df_train, df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8gWUxkeISZn"
      },
      "outputs": [],
      "source": [
        "df_train, df_test = reach_desired_distribution(df_train, df_test)\n",
        "percentage_train, percentage_test = get_distribution(df_train, df_test)\n",
        "print(f\"Pourcentage de textes d'entraînement : {percentage_train:.2f}%\")\n",
        "print(f\"Pourcentage de textes de test : {percentage_test:.2f}%\")\n",
        "num_copy = check_copy(train_texts, test_texts)\n",
        "print(f\"Il y a {num_copy} textes en commun entre les données d'entraînement et de test.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4npUOv9ISZo"
      },
      "outputs": [],
      "source": [
        "df_train.to_csv(\"data/train.csv\", index=False)\n",
        "df_test.to_csv(\"data/test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJI8qKZhISZo"
      },
      "source": [
        "## Visualisation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1pdVVNjISZo"
      },
      "source": [
        "Est-ce que les classes sont représentées de manière égale entre les deux ensembles ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9Ocrw1BISZo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compare_class_distribution(df_train, df_test):\n",
        "\n",
        "    total_train = len(df_train)\n",
        "    total_test = len(df_test)\n",
        "\n",
        "    train_distribution = {source: count / total_train * 100 for source, count in df_train.groupby(\"valeur\").size().to_dict().items()}\n",
        "    test_distribution = {source: count / total_test * 100 for source, count in df_test.groupby(\"valeur\").size().to_dict().items()}\n",
        "\n",
        "    x = np.arange(len(train_distribution))\n",
        "    width = 0.4\n",
        "    train_bars = plt.bar(x - 0.2, train_distribution.values(), width, label=\"Ensemble de train\")\n",
        "    test_bars = plt.bar(x + 0.2, test_distribution.values(), width, label=\"Ensemble de test\")\n",
        "\n",
        "    for bar in train_bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width() / 2, height, f'{round(height, 2)}', ha='center', va='bottom')\n",
        "\n",
        "    for bar in test_bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width() / 2, height, f'{round(height, 2)}', ha='center', va='bottom')\n",
        "\n",
        "    plt.xticks(x, train_distribution.keys(), rotation=90)\n",
        "    plt.xlabel(\"Parti politique\")\n",
        "    plt.ylabel(\"Pourcentage de documents\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "compare_class_distribution(df_train, df_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD7o_qMDISZp"
      },
      "source": [
        "Les répartitions sont assez égalitaires !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW9frkUpISZp"
      },
      "source": [
        "\n",
        "||   ELDR    | GUE/NGL | PPE-DE     | PSE | Verts/ALE\n",
        "|:---| :---        |    :----   |          :--- | :--- | :--- |\n",
        "|Nombre de documents| 3 346 | 4 482   |  11 429 | 9 066 | 3 961 |\n",
        "|Pourcentage de documents|10.33%|13.84%|35.29%|27.99%|12.23%|\n",
        "\n",
        "On a donc un corpus sans doublons mais avec une répartition et une distribution des classes et des documents fidèle au corpus originel !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxe6vnGMISZp"
      },
      "source": [
        "# Pé-traitement des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a72UjdeaISZp"
      },
      "source": [
        "## - Normalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-ruWMLUISZp"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aDgfCu2ISZp"
      },
      "outputs": [],
      "source": [
        "def lemmatisation(text):\n",
        "    doc = nlp(text)\n",
        "    return \" \".join([token.lemma_ for token in doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYE4p_BjISZp"
      },
      "outputs": [],
      "source": [
        "def i_love_lowercase(text):\n",
        "\n",
        "    return text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbq3BzB8ISZp"
      },
      "outputs": [],
      "source": [
        "def remove_stop_words(text):\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "\n",
        "    return \" \".join(filtered_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3mHEfKuISZr"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(text):\n",
        "\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap12Qq7_ISZr"
      },
      "outputs": [],
      "source": [
        "def get_beautiful_clean_text(text):\n",
        "\n",
        "    text = i_love_lowercase(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = remove_stop_words(text)\n",
        "    text = lemmatisation(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "df_train[\"texte_nettoyé\"] = df_train[\"texte\"].map(get_beautiful_clean_text)\n",
        "df_test[\"texte_nettoyé\"] = df_test[\"texte\"].map(get_beautiful_clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixt26YgqISZr"
      },
      "outputs": [],
      "source": [
        "df_train.to_csv(\"data/train.csv\", index=False)\n",
        "df_test.to_csv(\"data/test.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZwTxmU7ISZr"
      },
      "outputs": [],
      "source": [
        "\n",
        "#### VOUS POUVEZ LANCER À PARTIR D'ICI EN IMPORTANT LES CSV ####\n",
        "df_train = pd.read_csv(\"data/train.csv\")\n",
        "df_test = pd.read_csv(\"data/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDVgRBl2ISZr"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2U8vWjlISZs"
      },
      "source": [
        "## - Vectorisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYVLow7uISZs"
      },
      "source": [
        "Dans l'article, on voit que les paramètres de la vectorisation jouent un rôle assez important.\n",
        "Nous allons donc essayer de tester cela avec le tfidf et le count vectorizer sur le KNN, l'algorithme utilisé dans l'article.\n",
        "Nous allons vectorisser de manière différentes nos données et stocker le tout dans une df assez grande. NOus allons ensuite tester le tout sur le KNN pour voir comment cela impacte nos données et jusqu'à ou on peut aller !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTAVpdZqISZs"
      },
      "source": [
        "## - Organisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljU9WZNPISZs"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhCpky4pISZt"
      },
      "outputs": [],
      "source": [
        "vectorizers = {\n",
        "    \"TfidfVectorizer\": {\n",
        "        \"model\": TfidfVectorizer(),\n",
        "        \"hyperparameters\": {\n",
        "            \"max_df\": [0.5, 0.9, 1], # ça c'est pour ignorer les mots qui appraîssent dans 0.5 ou 0.9 des documents #defaut = 1\n",
        "            \"max_features\": [1000, 5000, 10000, 15000], # ça c'est pour limiter le nombre de mots à 1000 ou 2000 #defaut = None\n",
        "            \"ngram_range\": [(1, 1), (1, 2)], # ça c'est pour prendre en compte les unigrammes ou les bigrammes #defaut = (1, 1)\n",
        "        }\n",
        "    },\n",
        "    \"CountVectorizer\": {\n",
        "        \"model\": CountVectorizer(),\n",
        "        \"hyperparameters\": {\n",
        "            \"max_df\": [0.8, 0.9],\n",
        "            \"max_features\": [1000, 5000, 10000, 15000],\n",
        "            \"ngram_range\": [(1, 1), (1, 2)],\n",
        "        }\n",
        "    }\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqEvYsGdISZt"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "\n",
        "def hyperparameter_vectorizer(vectorizers):\n",
        "\n",
        "    param_grid = vectorizers[\"hyperparameters\"]\n",
        "    list_vectorizers = []\n",
        "\n",
        "\n",
        "    if len(param_grid) == 1:\n",
        "        param_name, param_values = list(param_grid.items())[0]\n",
        "        for i, value in enumerate(param_values, start=1):\n",
        "            params = {param_name: value}\n",
        "            vectorizer = vectorizers[\"model\"].__class__(**params)\n",
        "            list_vectorizers.append(vectorizer)\n",
        "\n",
        "    else:\n",
        "        param_grid_combinations = list(product(*param_grid.values()))\n",
        "        for i, combination in enumerate(param_grid_combinations, start=1):\n",
        "            params = {param_name: value for param_name, value in zip(param_grid.keys(), combination)}\n",
        "            vectorizer = vectorizers[\"model\"].__class__(**params)\n",
        "            list_vectorizers.append(vectorizer)\n",
        "\n",
        "    return list_vectorizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuubHG4YISZt"
      },
      "outputs": [],
      "source": [
        "list_vectorizers = hyperparameter_vectorizer(vectorizers[\"TfidfVectorizer\"])\n",
        "list_vectorizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8xuMfrgISZu"
      },
      "source": [
        "## - Vectorisation des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bfaGxHcISZu"
      },
      "outputs": [],
      "source": [
        "def store_vectorisations(list_vectorizers, x_train, x_test):\n",
        "\n",
        "    all_vectorisations = []\n",
        "\n",
        "    for i, vectorizer in enumerate(list_vectorizers):\n",
        "        x_train_vectorized = vectorizer.fit_transform(x_train)\n",
        "        x_test_vectorized = vectorizer.transform(x_test)\n",
        "        all_vectorisations.append({\"vectorizer_id\": vectorizer.__class__.__name__ + str(i),\n",
        "                                   \"vectorizer\": vectorizer,\n",
        "                                   \"x_train\": x_train_vectorized,\n",
        "                                   \"x_test\": x_test_vectorized})\n",
        "\n",
        "    df_vectorisations = pd.DataFrame(all_vectorisations)\n",
        "\n",
        "    return df_vectorisations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9aT3YsOISZu"
      },
      "outputs": [],
      "source": [
        "df_vectorisations = store_vectorisations(list_vectorizers, df_train[\"texte_nettoyé\"], df_test[\"texte_nettoyé\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esN9LobGISZu"
      },
      "source": [
        "## - Test avec KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui52nnvQISZu"
      },
      "outputs": [],
      "source": [
        "def im_counting_on_you_knn(df_vectorisations, y_train, y_test):\n",
        "\n",
        "    model = KNeighborsClassifier(n_neighbors=1)\n",
        "    scores = []\n",
        "\n",
        "    for i, row in df_vectorisations.iterrows():\n",
        "        vectorizer = row[\"vectorizer\"]\n",
        "        x_train = row[\"x_train\"]\n",
        "        x_test = row[\"x_test\"]\n",
        "        model.fit(x_train, y_train)\n",
        "        y_pred = model.predict(x_test)\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "        scores.append({\"vectorizer\": vectorizer, \"scores\": report})\n",
        "\n",
        "    return scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jK4m2Z_ISZv"
      },
      "outputs": [],
      "source": [
        "scores = im_counting_on_you_knn(df_vectorisations, df_train[\"valeur\"], df_test[\"valeur\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QF13OcIISZv"
      },
      "outputs": [],
      "source": [
        "sorted_scores = sorted(scores, key=lambda x: x[\"scores\"][\"accuracy\"], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Brnw8pPISZv"
      },
      "outputs": [],
      "source": [
        "for score in sorted_scores:\n",
        "    print(f\"Vectorizer: {score['vectorizer'].__class__.__name__}\")\n",
        "    for key, value in score[\"scores\"].items():\n",
        "        print(f\"\\t{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ieBOO7gISZv"
      },
      "source": [
        "Résultats médiocres et le model avec la meilleure accuracy a des fscores désastreuses. PLutôt utiliser ces dernières pour l'évaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9wafRM9ISZv"
      },
      "source": [
        "# Comparaison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeL3_JmVISZw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "def time_function(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        return result, end - start\n",
        "    return wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lnTLkQ6ISZw"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "\n",
        "    # Contient tous les modèles entrainés !\n",
        "    all_models = []\n",
        "\n",
        "    def __init__(self, name: str, model=None, vectorisation=None, scores={}, running_time=None, performance=None):\n",
        "        self.name = name\n",
        "        self.model = model\n",
        "        self.vectorisation = vectorisation\n",
        "        self.scores = scores\n",
        "        self.running_time = running_time\n",
        "        self.performance = performance\n",
        "\n",
        "        Model.all_models.append(self)\n",
        "        # Pour chaque modèle entraîné, on garde son nom (algo + num), le modèle et ses\n",
        "        # hyperparamètres pour cet entraînement, les métriques obtenues, et son temps d'execution !\n",
        "\n",
        "    # Pour vider la liste si nécessaire !\n",
        "    @classmethod\n",
        "    def reset(cls):\n",
        "        cls.all_models = []\n",
        "\n",
        "    # Entraîne le modèle\n",
        "    @time_function\n",
        "    def fit_model(self, x_train, y_train):\n",
        "        self.model.fit(x_train, y_train)\n",
        "\n",
        "    # Prédictions et scores\n",
        "    @time_function\n",
        "    def predict(self, x_test, y_test):\n",
        "        y_pred = self.model.predict(x_test)\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        accuracy_dict =  {\"accuracy\": accuracy, **report}\n",
        "        self.scores = accuracy_dict\n",
        "        return y_pred\n",
        "\n",
        "      # Lance les fonctions d'entraînement et de prédiction\n",
        "    # Stock les temps d'execution\n",
        "    def test_model(self, x_train, y_train, x_test, y_test):\n",
        "        _, fit_execution_time = self.fit_model(x_train, y_train)\n",
        "        y_pred, predict_execution_time = self.predict(x_test, y_test)\n",
        "        self.running_time = fit_execution_time + predict_execution_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGDTMAxvISZw"
      },
      "outputs": [],
      "source": [
        "Model.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx2RJhJVISZw"
      },
      "outputs": [],
      "source": [
        "models_dict = {\n",
        "    \"LogisticRegression\": {\"model\": LogisticRegression(), \"hyperparameters\": {\"C\": [0.5, 1.0]}},\n",
        "    # \"SGDClassifier\": {\"model\": SGDClassifier(), \"hyperparameters\": {\"alpha\": [0.0001, 0.001, 0.01], \"loss\": [\"hinge\", \"squared_hinge\"]}},\n",
        "    \"LinearSVC\": {\"model\": LinearSVC(), \"hyperparameters\": {\"C\": [0.5, 1.0], \"dual\":[True, False]}},\n",
        "    # \"SVC\": {\"model\": SVC(), \"hyperparameters\": {\"kernel\": [\"poly\", \"sigmoid\"]}},\n",
        "    \"RidgeClassifier\": {\"model\": RidgeClassifier(), \"hyperparameters\": { \"alpha\": [1.0, 1.5, 2]}},\n",
        "    \"LGBMClassifier\": {\"model\": LGBMClassifier(), \"hyperparameters\": {\"max_depth\": [1000], \"n_estimators\": [1000]}},\n",
        "    \"RandomForestClassifier\": {\"model\": RandomForestClassifier(), \"hyperparameters\": {\"max_depth\": [100]}}\n",
        " }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7Iszv_9ISZx"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "\n",
        "def hyperparameters_training(model_name, model_dict, x_train, y_train, x_test, y_test):\n",
        "    param_grid = model_dict[\"hyperparameters\"]\n",
        "\n",
        "    if len(param_grid) == 1:\n",
        "        param_name, param_values = list(param_grid.items())[0]\n",
        "        for i, value in enumerate(param_values, start=1):\n",
        "            params = {param_name: value}\n",
        "            model = model_dict[\"model\"].__class__(**params)\n",
        "            model_obj = Model(name=f\"{model_name}_{i}\", model=model)\n",
        "            model_obj.test_model(x_train, y_train, x_test, y_test)\n",
        "    else:\n",
        "        param_grid_combinations = list(product(*param_grid.values()))\n",
        "        for i, combination in enumerate(param_grid_combinations, start=1):\n",
        "            params = {param_name: value for param_name, value in zip(param_grid.keys(), combination)}\n",
        "            model = model_dict[\"model\"].__class__(**params)\n",
        "            model_obj = Model(name=f\"{model_name}_{i}\", model=model)\n",
        "            model_obj.test_model(x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV0e78lQISZx"
      },
      "outputs": [],
      "source": [
        "@time_function\n",
        "def test_models(models, x_train, y_train, x_test, y_test):\n",
        "\n",
        "    for model_name, model_dict in models.items():\n",
        "        print(f\"On teste le modèle {model_name} !\")\n",
        "        hyperparameters_training(model_name, model_dict, x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzm0xg2aISZx"
      },
      "outputs": [],
      "source": [
        "def test_models_and_vectorisations(models, vectorisations, df_train, df_test):\n",
        "\n",
        "    for i, row in vectorisations.iterrows():\n",
        "        print(f\"Vectorisation {row['vectorizer'].__class__.__name__} en cours !\")\n",
        "        x_train = row[\"x_train\"]\n",
        "        x_test = row[\"x_test\"]\n",
        "        test_models(models, x_train, df_train[\"valeur\"], x_test, df_test[\"valeur\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyFlD8rJISZx"
      },
      "outputs": [],
      "source": [
        "test_models_and_vectorisations(models_dict, df_vectorisations, df_train, df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNzLHyeHISZy"
      },
      "outputs": [],
      "source": [
        "##### POUR VISUALISER TOUS LES RÉSULTADOS ! #####\n",
        "from prettytable import PrettyTable\n",
        "def table_results(models):\n",
        "\n",
        "    bests_of_the_bests = PrettyTable([\"Model\", \"hyperparametres\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"Running Time\"])\n",
        "\n",
        "\n",
        "    for model_obj in models:\n",
        "        if \"accuracy\" in model_obj.scores.keys():\n",
        "            bests_of_the_bests.add_row([model_obj.name, model_obj.model, model_obj.scores[\"accuracy\"], model_obj.scores[\"macro avg\"][\"precision\"], model_obj.scores[\"macro avg\"][\"recall\"], model_obj.scores[\"macro avg\"][\"f1-score\"], model_obj.running_time])\n",
        "\n",
        "    return bests_of_the_bests\n",
        "\n",
        "table_results(Model.all_models)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "app_auto_dm_test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}